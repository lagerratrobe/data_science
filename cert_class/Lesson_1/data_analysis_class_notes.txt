Data Analysis Class - 10/2/2017

Introduction:
  Around 40 people in the class, varied set of backgrounds, most people already working and looking to expand their skillsets.  A few poeple wanting to pivot, or looking for a new career. Several people mentioned being in positions where they are "swimming in data" and need to have more skills in order to unblock themselves. Also several people who are not specifically in Analytic roles, but who have to do analysis as parts of their day-to-day lives.
  
Course Requirements: (3 books)
  - Statistics in a Nutshell
  - What is a P-Value Anyway?
  - R for Data Science
  
Class overview:
- 10 weeks of clss
- Think about projects to do with skills from this class
- There is a final project (can be done individually)

Changes in Market Trends
- Historically, it was enough to do retrospective analysis to figure out "what happened"
- Now however, companies would like to be able to predict what will happen in the futore and make recommendations

Data Aanlytics Tool Box:
- R and Python
- SQL
- Machine learning

(Train and retrain models continuously)

Excel exercise:
- Seatmate is named "Barb"

R Statistical Programming Language
- Similar to MatLab
- More mature for statistical analysis than Python, but Python is catching up

Data Mining to Machine Learning

Supervised learning : regression and classification

Unsupervised learning: clusterning

Linear regression
Logistic regressions
Decision trees
Vector machines
Neural Networks

Overview of the Iterative Data Analysis Process:
  Ask questions > Wrangle Data > Explore Data > Draw conclusion > Communicate findings
  
Ask Questions:
  Why did sales rise in the past few years?
  
Wrangle data:
  Examples of data types that need to be wrangled:
    Dates
    Currency
    No-data

Excel work: (Use vehicle data provided in class)
  Which car is most efficient
  Are any of the variables related to each other

----------------

Class 2: Intro to Graphical Data Anlysis

- Recap of Day 1:

Sometimes you don't go through all of the Data Analysis steps.  You might stop at the Data Exploration or even Wrangling step because you find that your data is no good.

- R: made of packages, only comes with a few basic ones

- To install:
  install.packages("package name")

- To use/load a package:
  library(<package>)
(equivalent to "import" statement in Python) loads package into memory

- Alternate way to call function:
  package_name::function_name()

- To control the size of a plot, use the repr package:
  library(repr)
  options(repr.plot.width=4, repr.plot.height=4)

- To select a subset of rows with a matching condition, use the filter() command.
  filter(mpg, cyl == 5)
This selects the cars in the data set that have 5 cylinders


--------------------
LESSON 3: 10/16/2017
--------------------

- Project Goals:
Question, Data Set, Team/Self, 3-7 pg. Powerpoint
Talk about all 5 stages
Top 3 problems you found and solved
Presentation peer rating

- Factors are an very interesting concept.  Let's you create classes that can be numerically compared to one-another.
  my_class <- as_factor(mpg$class)

- Regression in R: (INstructor segway)
my_model <- lm(mpg~., data=mpg)

Gets linear relationship between mpg and all other variables

- How to read CSV files:
    read_csv() which is part of tidyverse::readr
    read_tsv() for tab-separated values
    read_delim() for custom delimiter text files
(There is also a read.csv() function that is part of the base R install)

- How to write out files:
    write_csv()
    write_tsz()
    write_excel()

-----------------------------
Lesson 4: Data Transformation
-----------------------------
Review of last week's homework:
- months as factor, they can cycle as in month 1 of this year versus month 1 of next year
- No easy way to numerically compare observations with factors, and sometimes you want to (ie number of bedrooms)
- In the class exercise, the ID field could be removed, since a random unique identifier is not really needed to do the data analysis.

- str_detect(data, <regex>): works by creating a vector index of all the positions within a string that match the regular expression.

ERD: (Entity relation Diagram) shows relationship between several DB tables

"Slowly changing dimensions": track movement and changes in data (need to look this up)

------------------------------
Lesson 5: Data Transformation
------------------------------
- Bargaining for team leads - no thanks!
- Went over the class project
- Yes/No question is a type of A/B test where you evaluate which option is best, A or B
- Everything related to obeservation is stored in a single row in Statistically normalized  data
- Loops can usually be converted into vectorized operations which are faster

- Look up "star schema"

---------------------
Lesson 6: Statistics
---------------------
- "inferential" just means that you're working with a sample
- histogram breaks up values into buckets or bins
- The longer the tail, the more the ditribution is skewed in that direction
- Look up how to calculate an absolute value  (square-root of squared values)
- Variance is looking at the sum of the differences between a specific point and the mean
- What the hell is "Mean Absolute Deviation"?
The steps to find the MAD include:
1. find the mean (average)
2. find the difference between each data value and the mean
3. take the absolute value of each difference
4. find the mean (average) of these differences
- KNN Classifier (Nearest Neighbor)

- P value less than 5% is what we want, otherwise it indicates that variable is not meaningful

---------------------
Lesson 7: Descriptive Statistics
---------------------
- Central tendencies:
    - Mode is useful to show factors as it will report the factor that is used the most
    - Median is useful for ordinal data like a Pain Scale
    - Mean is useful for symetrical data, when skewed, median is better
- sqrt((x1 - x2) **2) or absolute differences are often used to describe a data value
- Normal Distribution:
    95% of values fall within 2 std deviations of mean
    99% of values fall within 3 std deviations of mean
- Probablity distribution:
    plot(pnorm(seq)~seq) R function plots
- Error = random error + bias
    Low random errom + high bias means data is tightly clustered, but there are bias regions which are clustered not around central tendency
    High random error + low bias means data will be scattered all over the place
 - Determining sample size to achieve statistical significance
    n = (2 * std_dev)/ effect or difference you want to be within  NOTE: confidence is represnted by numbers of std_dev
- degrees of freedom : easy way to do it is (sample size - number of variables)
- stratify data: how to split data up into different pieces, ie split housing prices by zipcode
- Data Sampling:
    sample(data, size = 100, replace = FALSE) - R function to select a 100 value sample from a data set
- Confidence interval is usually acceptable around 95%, but it depends on application.  99% is needed for things like airplane safety.

- the p-value is the probability that when the null hypothesis is true, the statistical summary would be the same as, or of greater magnitude, than the actual observed results.

---------------------------
Lesson 8 - Machine Learning
---------------------------
- Statistics review:
  * Inferentian vs. Descriptive stats.
  * Distance: confidence interval, Key Nearest Neighbor using Euclidean Distance. Probabilistic distance also.
  * Distribution: tells us the probability that something will be near the mean
  * Confidence Interval:

Machine Learning definitions:
- Data mining: allow you to discover patterns in data
- machine learning: algorthms to predict/decide using the data patterns
- Biz intel: Extracting info from data
- Analytics: manual discovery and communication of data patterns
- Data Science: Extracting knowledge from data

Data Mining:
- all about patterns, aka "mathematical model"

Machine Learning:
- machines learn or train models from data
- trained models are used to make forecasts

Linear regression is an example of data mining, machine learning and statistics, all combined

Degrees of freedom come into play when you are trying to create a linear regression model to fit different points
- General linear additive models: using more than just x and y,

- Supervised models learn from marked cases which contain a label that includes the true value that we'd like to predict in the future.  Training helps minimize the error between the model prediction and the label value.

- Data quality and structure problems can make supervised learning difficult.
  * Insufficient data
  * Poor data quality
  * Unrepresentative
  * Unimportant
  * Collinear features

- Underfitting and overfitting
  * Overfit: can't be generalied. Learned a specific data set with zero error
  * Underfit model: only using one or 2 coefficients

- Regression model: predict numeric values
- Classifiers: predict categories

---------------------
Penn State Stats pages
---------------------
https://onlinecourses.science.psu.edu/stat100/node/35

---------------------
Lesson 8: Nov 27, 2017
----------------------

Presentation from Disney Consumer Experience (Chris Lomax, Katie Jo Giesbers)  Both Directors in this division.

"in 2008, tablets weren't even considered as a factor in entertainment"
- Starwave purchased in 1998. Today Seattle is the tech hub for Disney with over 800 people. Focus on digital streaming of full episodes.  Trying to cater to the "cord-cutters".
- Areas of Focus for this team:
    * Inventory Mgt: How many ads can they serve to their clients? Helped them understand how the business was working.  Figure out how many times a specific ad can be displayed, on what channels, in what areas, during what shows, etc.

    * Forecasting:   Portfolio-level forecasting. Large-scale forecasting.  Things like changing the autentication process completely change the assumptions in the models use to forecast behavior. They work with a science team to do the modeling.  They have never-ending change in their models and analysis. Digital data is not ideal, much cookie-based data, which isn't very good, especially with user-level data.  Digital streaming is not growing on traditional platforms in the same way it is on things like YouTube and Facebook.  Bar chart with color (stacked bars) used to show predicted trends.

    * BI Reporting:  Tableau exploited as a product to present data to executives.  Self-service analytics and dashboards. Important to provide context and color - tell a story. What's happening, and it's expected - or not.  Tips for getting users to use self-service BI. Dashboards with too much shit in them was a problem.  People had no idea how to use filters, etc.  Things need to be broken up into smaller, focused chunks that are self-explanatory.  "Don't just buold what they want!"

    * Data Management/Quality:  They don't like having this role, but they have to understand the data that goes into the forecasts.  They usually are the ones that find bugs in the data. They get alot of data from other parties, like HuLu. An example of a problem they caught was excessive usage stats from TV's that didn't get turned off and were reporting that users were watching 60 hours straight of TV.  Effect was that companies would be charged for advertising when in fact a real person had not seen the ad.

    * Strategic Projects: What is the impact of the proposed changes that the excutive team is thinking of making? What would happen if they only had the last season of "Gray's Anatomy" available online?  Ad Repetition Analyis: Wanted to see how bad the repetition was on two platforms from 10/2015 - 08/2016. Checked raw log files of 300 Million records. Question 1 - What is a repeated ad? Is it the same ad, 2 ads for the same company, etc?  Findings: Very few ads show more than 3x in an episode. BUT 50% of ads are repeats from one episode to another.  Most of the repeats happen right away (after the prior pod). Out of 17 ads, only 8 were unique.

--------------------
Lesson 9
-------------------
- Unsupervised learning:
    * Does not require labels
    * KNN K=3 "the 3 nearest neighbors" (Can use Euclidean or Manhattan distance)
    * K-means Clustering (Lloyd's Algorithm)
- Lots of clustering algorithms
- Finding the right number of clusters (k versus WCCS)

----------------------------------------------
Introduction to Data Visualizations Essentials
----------------------------------------------

Course introduction: Sounds like there will be mostly Excel, Power BI and R in this clas. Fine with me.

1. Project Dev. and Planning:
- Visualization is the "art of storytelling with numbers and graphics"
- Perspective, People, Questions, KPI's (Key Performance Indicators)

2. Viz projects are often slightly different from Software Dev projects in that they:
- have greater ambiguity
- high expectations
- many interested stakeholders
- many uncontrollable factors

3. Framework for Viz Projects: (How to approach a new project succesfully)
- Perspective: bring outside perspective to the project
- Keep bias out of the project in early phases
- Be flexible in considering a change in project scope

What people to include:
- Manager (for connections and cross-team access)
- Connect with Infrastructure group for support
- Have a kickoff meeting (what do you cover in the kickoff?)

Ask the right questions to help you determine what the client needs
- What is their main day-to-day and long term focus
- What metrics do they currently use
- What are their current pain points
- How will your report/product be used in their dauly work and in decision making
- What databases do they currently use (and if possible speak to their DBA)

Come up with the right KPIs
- Usually these are a measurement-per-unit
- Frequently used KPI
- Industry specific KPI
- Use standard descriptions like Mean, Median, Mode, etc
- Name your metrics
- Document the formulas you use and get peer review

-----------------------------------------------------
Dirty secrets about metrics - http://www.itsmsolutions.com/newsletters/DITYvol5iss33.htm
-----------------------------------------------------
Using the trip analogy, the goal might be to get across the desert. Goals are extending, and should grow an organization. In this analogy a goal is the destination. Critical Success Factors (CSF) are things you have to do to achieve the goal, CSFs are attainable. One CSF from our analogy might be to avoid overheating. A KPI is a measure of some property of a CSF. The KPI in this analogy might be water temperature, as expressed by a temperature gauge. By watching the water temperature gauge we have an indication if we are likely to meet the requirement to avoid overheating, and if we do not overheat, we stand a good chance of driving across the desert.

If we were not crossing the desert in a car then our CSF and KPI would probably be quite different. For example, if we were flying a plane across the dessert instead of driving a car through it, we would probably also want to know about air speed and altitude, and water temperature might not be required.

KPI metrics (like water temperature) derive from CSFs (like avoiding overheating). CSFs derive from goals (like driving a car across the desert). Goals derive from current process maturity and business/mission requirements. Every goal will probably have unique CSFs, which in turn probably have unique KPIs.

----------------------------------------------------------
Using Key Performance Indicators in Dashboards - https://www.stickyminds.com/article/building-software-development-dashboards-key-performance-indicators
----------------------------------------------------------
Key performance indicators help managers gauge the team’s progress, understand what phase the project is in, and figure out where costs, goals, or processes need to be adjusted. This article details some typical KPIs to be used in dashboards to provide business analytics and communicate information in the most useful way.

When you are giving a presentation and showing analytic information, the charts should speak for themselves. Any picture, graph, or chart you need to explain is not clear enough.

----------------------------------------------------------
Elements of a great Scrum Team - https://www.infoq.com/articles/great-scrum-team#
----------------------------------------------------------

---------------------------------
DV2 Class Notes - SQL
---------------------------------

sqlite> .schema laptops
CREATE TABLE laptops(
  "id" TEXT,
  "model" TEXT,
  "speed" TEXT,
  "ram" TEXT,
  "hdd" TEXT
);
sqlite> .schema products
CREATE TABLE products(
  "maker" TEXT,
  "model" TEXT,
  "type" TEXT
);

List all the laptops made by maker 'a' and 'b' - include maker, model, id, speed, ram and hdd

select products.maker, products.model, products.type, laptops.speed, laptops.ram, laptops.hdd FROM products left outer join laptops on products.model = laptops.model where products.type = 'Laptop' and products.maker in ('a', 'b');